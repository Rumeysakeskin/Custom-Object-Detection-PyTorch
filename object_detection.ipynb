{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNryegZZutm3KyUo/6bc5IC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumeysakeskin/Custom-Object-Detection-PyTorch/blob/main/object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4musg2mkxURo"
      },
      "outputs": [],
      "source": [
        "# Install and unzip dataset\n",
        "import zipfile, urllib.request, shutil\n",
        "url = \"https://github.com/Stroma-Vision/machine-learning-challenge/releases/download/v0.1/challenge.zip\" \n",
        "file_name = 'challenge.zip'\n",
        "\n",
        "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "    shutil.copyfileobj(response, out_file)\n",
        "    with zipfile.ZipFile(file_name) as zf:\n",
        "        zf.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import glob as glob\n",
        "import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "xkIk40nbOHe_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE IMAGE DATASET FROM VIDEO"
      ],
      "metadata": {
        "id": "EHM24oXb0R2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_image_dataset(data_dir, data_path, annotations):\n",
        "\n",
        "    frame_counter = 0\n",
        "    ret = True\n",
        "    \n",
        "    # Load the video and extract the frames\n",
        "    capture = cv2.VideoCapture(data_dir)\n",
        "    \n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) #test: 1800 frames\n",
        "\n",
        "    # Calculate the interval between frames to keep\n",
        "    # Determine which frames to keep and which to discard based on the interval value\n",
        "    # interval = int(capture.get(cv2.CAP_PROP_FPS) / FPS) # FPS = 30\n",
        "\n",
        "    while ret: # Break the loop if the video has ended\n",
        "        ret, frame = capture.read() # frame shape 640x640x3\n",
        "        if not ret:\n",
        "          break\n",
        "        frame_name = annotations[\"images\"][frame_counter]['file_name']\n",
        "        cv2.imwrite(f'{data_path}/{frame_name}', frame)\n",
        "        frame_counter +=1\n",
        "\n",
        "    print(f\"{total_frames} frames saved to {data_path}\")\n",
        "\n",
        "        # plt.imshow(frame[:,:,::-1])  # Plot frames\n",
        "        # plt.show()        "
      ],
      "metadata": {
        "id": "JZ4tmNXQxqR-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load video dataset and annotation files\n",
        "def load_annotations(file_path):\n",
        "  with open(file_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "  return annotations\n",
        "  \n",
        "annotations = [(\"test\", \"/content/challenge/annotations/instances_test.json\"),\n",
        "               (\"val\", \"/content/challenge/annotations/instances_val.json\"),\n",
        "               (\"train\", \"/content/challenge/annotations/instances_train.json\")]\n",
        "\n",
        "videos = [(\"test\", \"/content/challenge/images/test/test.mp4\"),\n",
        "               (\"val\", \"/content/challenge/images/val/val.mp4\"),\n",
        "               (\"train\", \"/content/challenge/images/train/train.mp4\")]\n",
        "\n",
        "for annotation in annotations:\n",
        "    name, path = annotation\n",
        "    locals()[f\"{name}_annotations\"] = load_annotations(path)\n",
        "\n",
        "for video in videos:\n",
        "    name, path = video\n",
        "    locals()[f\"{name}_data\"] = path"
      ],
      "metadata": {
        "id": "1E07vL1EmbLO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image data folders from videos\n",
        "data_dirs = [\"train_data\",\"test_data\",\"val_data\"]\n",
        "if not os.path.exists('./frames'):\n",
        "    os.mkdir('./frames/')\n",
        "    for data_dir in data_dirs:\n",
        "      os.mkdir(f'./frames/{data_dir}/')  \n",
        "\n",
        "# Create images from video frames\n",
        "create_image_dataset(train_data, \"./frames/train_data\", train_annotations) # data, data_path, annotations\n",
        "create_image_dataset(val_data, \"./frames/val_data\", val_annotations) \n",
        "create_image_dataset(test_data, \"./frames/test_data\", test_annotations) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_I1-a61KuCl",
        "outputId": "6a5f3349-ebab-4eb5-9af5-d0053b656133"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7200 frames saved to ./frames/train_data\n",
            "1800 frames saved to ./frames/val_data\n",
            "1800 frames saved to ./frames/test_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARE DATASET"
      ],
      "metadata": {
        "id": "1HSwVQTZ0O4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, data_dir, annotations, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.annotations = annotations\n",
        "        self.transform = transform\n",
        "        \n",
        "        # get all the image paths in sorted order\n",
        "        self.image_paths = glob.glob(f\"{self.data_dir}/*.jpg\")\n",
        "        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
        "        self.all_images = sorted(self.all_images)\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return len(self.annotations)\n",
        "        return len(self.all_images)\n",
        "        \n",
        "# loads a video and extracts frames from it\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_name = self.all_images[idx]\n",
        "        image_path = os.path.join(self.data_dir, image_name)\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) \n",
        "        image /= 255.0\n",
        "        image = torch.as_tensor(image)\n",
        "        image = np.transpose(image, (2, 0, 1)) #Image shape: tensor[(3x640x640)]\n",
        "\n",
        "        # Get the annotations for each frame\n",
        "        target = {}\n",
        "\n",
        "        x, y, w, h = self.annotations[\"annotations\"][idx]['bbox'] # [x, y, width, height] --> [x_min, y_min, x_max, y_max]\n",
        "\n",
        "        target[\"boxes\"] = torch.as_tensor([x, y, x + w, y + h], dtype=torch.float32).unsqueeze(0)  # Add a batch dimension\n",
        "        target[\"labels\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['category_id'] - 1], dtype=torch.int64) # Change the labels to 0 and 1 as there are 2 classes\n",
        "        target[\"image_id\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['image_id']])\n",
        "        target[\"area\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['area']], dtype=torch.float32)\n",
        "        target[\"iscrowd\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['iscrowd']], dtype=torch.int64)\n",
        "        # target[\"track_id\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['track_id']])\n",
        "       \n",
        "        # Apply the transformations if provided\n",
        "        # if self.transform is not None:\n",
        "        #   image = self.transform(image)\n",
        "        #   target = self.transform(target)\n",
        "       \n",
        "        return image, target"
      ],
      "metadata": {
        "id": "4F0wpCNykfaz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    The transform pipeline makes it possible to perform operations such as \n",
        "    normalization, data augmentation, and other preprocessing steps that can \n",
        "    help to improve the performance of a neural network during training.\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    To handle the data loading as different images may have different number \n",
        "    of objects and to handle varying size tensors as well.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "lGGL7w00L2hA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATALOADERS"
      ],
      "metadata": {
        "id": "S5rp6qu2zzPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset object for videos and their corresponding annotations\n",
        "\n",
        "train_dataset = VideoDataset(\"./frames/train_data\", train_annotations, get_transform(True))\n",
        "val_dataset = VideoDataset(\"./frames/val_data\", val_annotations, get_transform(False))\n",
        "\n",
        "# Define the dataloader load the data in batches during training and validation\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "43OPLdeUwLeb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FASTER RCNN MODEL"
      ],
      "metadata": {
        "id": "UGlV8MuNzrKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def fasterrcnn_model(num_classes):\n",
        "  # load a model pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "QrS9TDIMES__"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING AND VALIDATING"
      ],
      "metadata": {
        "id": "jVkJ0M8bzWgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_data_loader, model):\n",
        "    global train_itr\n",
        "    global train_loss_list\n",
        "    \n",
        "     # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "    \n",
        "    for i, data in enumerate(prog_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "        train_loss_list.append(loss_value)\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        train_itr += 1\n",
        "    \n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "    return train_loss_list\n",
        "\n",
        "def validate(valid_data_loader, model):\n",
        "    \n",
        "    global val_itr\n",
        "    global val_loss_list\n",
        "    \n",
        "    # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
        "    \n",
        "    for i, data in enumerate(prog_bar):\n",
        "        images, targets = data\n",
        "        \n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "        val_loss_list.append(loss_value)\n",
        "        val_itr += 1\n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "    return val_loss_list"
      ],
      "metadata": {
        "id": "CIOKGJCVI8lt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_EPOCHS = 2\n",
        "SAVE_MODEL_EPOCH = 2\n",
        "OUT_DIR = 'outputs'\n",
        "\n",
        "if not os.path.exists(OUT_DIR):\n",
        "    os.mkdir(OUT_DIR)\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "\n",
        "model = fasterrcnn_model(num_classes=NUM_CLASSES)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# Define criterion, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()  # standard crossentropy loss for classification\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # the scheduler divides the lr by 10 every 10 epochs\n",
        "\n",
        "train_itr = 1\n",
        "val_itr = 1\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
        "\n",
        "    train_loss = train(train_dataloader, model)\n",
        "    val_loss = validate(val_dataloader, model)\n",
        "\n",
        "    if (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n",
        "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
        "        print(f\"MODEL SAVED IN {OUT_DIR}/model{epoch+1}.pth ... \\n\")\n",
        "\n",
        "    # create two subplots, one for each, training and validation\n",
        "    figure_1, train_ax = plt.subplots()\n",
        "    figure_2, valid_ax = plt.subplots()\n",
        "\n",
        "    if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n",
        "        train_ax.plot(train_loss, color='blue')\n",
        "        train_ax.set_xlabel('iterations')\n",
        "        train_ax.set_ylabel('train loss')\n",
        "        valid_ax.plot(val_loss, color='red')\n",
        "        valid_ax.set_xlabel('iterations')\n",
        "        valid_ax.set_ylabel('validation loss')\n",
        "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
        "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPUNls--EiwE",
        "outputId": "7c420395-cd7a-4468-b34b-85af33de3c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 178MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1 of 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.2059:   1%|          | 3/450 [15:25<38:26:07, 309.55s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD TRAINED MODEL"
      ],
      "metadata": {
        "id": "9iL-03dJMoyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasterrcnn_model(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "model.load_state_dict(torch.load(f\"{OUT_DIR}/model{epoch+1}.pth\", map_location=DEVICE))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA-1wx5ifCQ2",
        "outputId": "3d2aa3e9-832a-498b-ab64-9545c1d553bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test instances: 1800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBJECT DETECTION INFERENCE ON VIDEO"
      ],
      "metadata": {
        "id": "mJ1R-KeltsSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = [\"bolt\", \"nut\"]\n",
        "\n",
        "# define the detection threshold...\n",
        "# ... any detection having score below this will be discarded\n",
        "detection_threshold = 0.8\n",
        "\n",
        "capture = cv2.VideoCapture(\"/content/challenge/images/test/test.mp4\") \n",
        "ret = True\n",
        "# Preparing variable for writer that we will use to write processed frames\n",
        "writer= None\n",
        "# Preparing variables for spatial dimensions of the frames\n",
        "h, w = None, None\n",
        "\n",
        "while ret: # Break the loop if the video has ended\n",
        "    ret, frame = capture.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    if w is None or h is None:  \n",
        "      h, w = frame.shape[:2]\n",
        "\n",
        "    image = frame\n",
        "    orig_image = image.copy()\n",
        "\n",
        "    image = torch.as_tensor(image)\n",
        "\n",
        "    image = np.transpose(image, (2, 0, 1)) # torch.Size([3, 640, 640])\n",
        "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
        "    image = torch.unsqueeze(image, 0) # torch.Size([1, 3, 640, 640]) add batch size\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "    \n",
        "    # load all detection to CPU for further operations\n",
        "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
        "    # carry further only if there are detected boxes\n",
        "    if len(outputs[0]['boxes']) != 0:\n",
        "        print(\"tt\")\n",
        "        boxes = outputs[0]['boxes'].data.numpy()\n",
        "        scores = outputs[0]['scores'].data.numpy()\n",
        "        # filter out boxes according to `detection_threshold`\n",
        "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "        draw_boxes = boxes.copy()\n",
        "        # get all the predicited class names\n",
        "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
        "        \n",
        "        # draw the bounding boxes and write the class name on top of it\n",
        "        for j, box in enumerate(draw_boxes):\n",
        "            cv2.rectangle(orig_image,\n",
        "                        (int(box[0]), int(box[1])),\n",
        "                        (int(box[2]), int(box[3])),\n",
        "                        (0, 0, 255), 2)\n",
        "            cv2.putText(orig_image, pred_classes[j], \n",
        "                        (int(box[0]), int(box[1]-5)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
        "                        2, lineType=cv2.LINE_AA)\n",
        "\n",
        "    # Writing processed frame into the file\n",
        "    # Initializing writer\n",
        "    # we do it only once from the very beginning when we get spatial dimensions of the frames\n",
        "    if writer is None:\n",
        "        # Constructing code of the codec to be used in the function VideoWriter\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "        # Writing current processed frame into the video file\n",
        "        writer = cv2.VideoWriter('output_test_video.mp4', fourcc, 30,\n",
        "                                 (orig_image.shape[1], orig_image.shape[0]), True)\n",
        "\n",
        "    # Write processed current frame to the file\n",
        "    writer.write(orig_image)\n",
        "\n",
        "# Releasing video reader and writer\n",
        "capture.release()\n",
        "writer.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "zqfjGRQYtU9F",
        "outputId": "74cde2a6-08de-401e-e679-886a9ebc115e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3e0d9ca6b33f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcapture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/challenge/images/test/test.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Preparing variable for writer that we will use to write processed frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Preparing variables for spatial dimensions of the frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
          ]
        }
      ]
    }
  ]
}