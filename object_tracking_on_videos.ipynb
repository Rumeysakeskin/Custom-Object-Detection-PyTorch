{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNctLkDWqiacY6+fIhSh23X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumeysakeskin/Custom-Object-Detection-PyTorch/blob/main/object_tracking_on_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4musg2mkxURo"
      },
      "outputs": [],
      "source": [
        "# Install and unzip dataset\n",
        "import zipfile, urllib.request, shutil\n",
        "\n",
        "url = \"https://ENTER_YOUR_LINK_HERE/YOUR_DATASET_NAME.zip\" \n",
        "file_name = 'YOUR_DATASET_NAME.zip'\n",
        "\n",
        "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "    shutil.copyfileobj(response, out_file)\n",
        "    with zipfile.ZipFile(file_name) as zf:\n",
        "        zf.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import glob as glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xkIk40nbOHe_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_image_dataset(data_dir, data_path, annotations):\n",
        "\n",
        "    frame_counter = 0\n",
        "    ret = True\n",
        "    \n",
        "    # Load the video and extract the frames\n",
        "    capture = cv2.VideoCapture(data_dir)\n",
        "    \n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) #test: 1800 frames\n",
        "\n",
        "    # Calculate the interval between frames to keep\n",
        "    # Determine which frames to keep and which to discard based on the interval value\n",
        "    # interval = int(capture.get(cv2.CAP_PROP_FPS) / FPS) # FPS = 30\n",
        "\n",
        "    while ret: # Break the loop if the video has ended\n",
        "        ret, frame = capture.read() # frame shape 640x640x3\n",
        "        if not ret:\n",
        "          break\n",
        "        frame_name = annotations[\"images\"][frame_counter]['file_name']\n",
        "        cv2.imwrite(f'{data_path}/{frame_name}', frame)\n",
        "        frame_counter +=1\n",
        "\n",
        "    print(f\"{total_frames} frames saved to {data_path}\")\n",
        "\n",
        "        # plt.imshow(frame[:,:,::-1])  # Plot frames\n",
        "        # plt.show()        "
      ],
      "metadata": {
        "id": "JZ4tmNXQxqR-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load video dataset and annotation files\n",
        "def load_annotations(file_path):\n",
        "  with open(file_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "  return annotations\n",
        "  \n",
        "annotations = [(\"test\", \"/content/challenge/annotations/instances_test.json\"),\n",
        "               (\"val\", \"/content/challenge/annotations/instances_val.json\"),\n",
        "               (\"train\", \"/content/challenge/annotations/instances_train.json\")]\n",
        "\n",
        "videos = [(\"test\", \"/content/challenge/images/test/test.mp4\"),\n",
        "               (\"val\", \"/content/challenge/images/val/val.mp4\"),\n",
        "               (\"train\", \"/content/challenge/images/train/train.mp4\")]\n",
        "\n",
        "for annotation in annotations:\n",
        "    name, path = annotation\n",
        "    locals()[f\"{name}_annotations\"] = load_annotations(path)\n",
        "\n",
        "for video in videos:\n",
        "    name, path = video\n",
        "    locals()[f\"{name}_data\"] = path"
      ],
      "metadata": {
        "id": "1E07vL1EmbLO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image data folders from videos\n",
        "data_dirs = [\"train_data\",\"test_data\",\"val_data\"]\n",
        "if not os.path.exists('./frames'):\n",
        "    os.mkdir('./frames/')\n",
        "    for data_dir in data_dirs:\n",
        "      os.mkdir(f'./frames/{data_dir}/')  \n",
        "\n",
        "# Create images from video frames\n",
        "create_image_dataset(train_data, \"./frames/train_data\", train_annotations) # data, data_path, annotations\n",
        "create_image_dataset(val_data, \"./frames/val_data\", val_annotations) \n",
        "create_image_dataset(test_data, \"./frames/test_data\", test_annotations) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_I1-a61KuCl",
        "outputId": "f2cca5c8-3d79-40e3-b3b0-191a6641455e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7200 frames saved to ./frames/train_data\n",
            "1800 frames saved to ./frames/val_data\n",
            "1800 frames saved to ./frames/test_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, data_dir, annotations, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.annotations = annotations\n",
        "        self.transform = transform\n",
        "        \n",
        "        # get all the image paths in sorted order\n",
        "        self.image_paths = glob.glob(f\"{self.data_dir}/*.jpg\")\n",
        "        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
        "        self.all_images = sorted(self.all_images)\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return len(self.annotations)\n",
        "        return len(self.all_images)\n",
        "        \n",
        "\n",
        "# loads a video and extracts frames from it\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_name = self.all_images[idx]\n",
        "        image_path = os.path.join(self.data_dir, image_name)\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) \n",
        "        image /= 255.0\n",
        "        image = torch.as_tensor(image)\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        \n",
        "        # Get the annotations for each frame\n",
        "        target = {}\n",
        "\n",
        "        # [x, y, width, height] --> [x_min, y_min, x_max, y_max]\n",
        "        x, y, w, h = self.annotations[\"annotations\"][idx]['bbox']\n",
        "\n",
        "        target[\"boxes\"] = torch.as_tensor([x, y, x + w, y + h], dtype=torch.float32).unsqueeze(0)  # Add a batch dimension\n",
        "        target[\"labels\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['category_id']], dtype=torch.int64)  \n",
        "        target[\"image_id\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['image_id']])\n",
        "        target[\"area\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['area']], dtype=torch.float32)\n",
        "        target[\"iscrowd\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['iscrowd']], dtype=torch.int64)\n",
        "        # target[\"track_id\"] = torch.as_tensor([self.annotations[\"annotations\"][idx]['track_id']])\n",
        "       \n",
        "        # Apply the transformations if provided\n",
        "        # if self.transform is not None:\n",
        "        #   image = self.transform(image)\n",
        "        #   target = self.transform(target)\n",
        "        # print(f\"image:{image}, target:{target}\")\n",
        "       \n",
        "        return image, target"
      ],
      "metadata": {
        "id": "4F0wpCNykfaz"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data transforms to be applied to the video frames\n",
        "# preprocess the input images in a dataset before feeding them into a neural network for training.\n",
        "# converts the input image from its original format to a PyTorch tensor.\n",
        "\n",
        "# the transform pipeline makes it possible to perform operations such as \n",
        "# normalization, data augmentation, and other preprocessing steps that can \n",
        "# help to improve the performance of a neural network during training.\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    # transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "lGGL7w00L2hA"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Create the dataset object for videos and their corresponding annotations\n",
        "\n",
        "train_dataset = VideoDataset(\"./frames/train_data\", train_annotations, get_transform(True))\n",
        "test_dataset = VideoDataset(\"./frames/test_data\", test_annotations, get_transform(False))\n",
        "val_dataset = VideoDataset(\"./frames/val_data\", val_annotations, get_transform(False))\n",
        "# Define the dataloader load the data in batches during training and inference\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "43OPLdeUwLeb"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "QrS9TDIMES__"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# Define criterion, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()  # standard crossentropy loss for classification\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # the scheduler divides the lr by 10 every 10 epochs"
      ],
      "metadata": {
        "id": "9E6Pju2i_EI0"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for running training iterations\n",
        "\n",
        "train_loss_list = []\n",
        "train_itr = 1\n",
        "def train(train_data_loader, model):\n",
        "    print('Training')\n",
        "    global train_itr\n",
        "    global train_loss_list\n",
        "    \n",
        "     # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "    \n",
        "    for i, data in enumerate(prog_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "        train_loss_list.append(loss_value)\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        train_itr += 1\n",
        "    \n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "    return train_loss_list"
      ],
      "metadata": {
        "id": "CIOKGJCVI8lt"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 20\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_loss = train(test_dataloader, model.to(DEVICE))\n",
        "\n",
        "    # update the learning rate\n",
        "    \n",
        "    # evaluate on the test dataset\n",
        "    # evaluate(model, data_loader_test, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "iPUNls--EiwE",
        "outputId": "0000bb9a-558b-4d00-8712-a0bea13766ef"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-fa471259e002>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}